{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2aa8af7-a07e-4128-b5ac-e0e652a9d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import openai\n",
    "import pickle\n",
    "import traceback\n",
    "import dotenv\n",
    "import logging\n",
    "import requests\n",
    "import re\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from time import sleep, time\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from openai import OpenAIError\n",
    "from data.serialize import SerializerSettings\n",
    "from models.utils import grid_iter\n",
    "from models.promptcast import get_promptcast_predictions_data\n",
    "from models.llmtime import get_llmtime_predictions_data\n",
    "from models.validation_likelihood_tuning import get_autotuned_predictions_data\n",
    "\n",
    "# Set up API key and environment\n",
    "# Create a file '.env' in the directory of this notebook and include 'OPENAI_API_KEY=<your key>'\n",
    "dotenv.load_dotenv(\".env\", override=True)\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a4e83d-8ebe-49f4-8584-7e254917bb90",
   "metadata": {},
   "source": [
    "# Run LLMTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2f3f89f-351c-4b95-aa94-f4d72b20105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM model name\n",
    "model_name = 'gpt-3.5-turbo-instruct'\n",
    "# Experiment starting date\n",
    "starting_date = pd.Timestamp('2021-11-01')\n",
    "# Assets to test on \n",
    "assets = ['gdaxi', 'ftse', 'cbu', 'SPGTCLTR', 'djci', 'SP500', 'stoxx', 'CRIX', 'cact']\n",
    "# Window sizes\n",
    "windows = [30, 45] # 60, 90, 120, 150\n",
    "# Number of predictions per step\n",
    "# Uses the 'n' parameter from GPT completions\n",
    "n = 128\n",
    "# Number of steps. Total daily predictions = n * steps\n",
    "steps = 8\n",
    "# Max retries per step\n",
    "# Usually no more than 2 - 3 are required\n",
    "max_retries = 50\n",
    "# Save results every 'save_interval' days\n",
    "save_interval = 1\n",
    "# Output directory. File names are generated automatically with the form '{date}_{asset}_{model_label}_w={window}.csv'\n",
    "output_directory = 'simulations_test'\n",
    "\n",
    "# LLMTime model hyperparameters, as outlined in the paper\n",
    "llm_hypers = dict(\n",
    "    temp=0.7,\n",
    "    alpha=0.95,\n",
    "    beta=0.3,\n",
    "    basic=False,\n",
    "    settings=SerializerSettings(base=10, prec=2, signed=True, time_sep=', ', bit_sep='', minus_sign='-')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0c4f119-b227-434c-bc99-2955590dca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must implement below for any other LLM\n",
    "def get_model_label(model_name):\n",
    "    if model_name == 'gpt-3.5-turbo-instruct':\n",
    "        return 'LLMTime GPT-3.5'\n",
    "    elif model_name == 'gpt-4':\n",
    "        return 'LLMTime GPT-4'\n",
    "    elif model_name == 'gpt-4o':\n",
    "        return 'LLMTime GPT-4o'\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unexpected model '{model_name}'.\")\n",
    "\n",
    "def get_model_hypers(model_name):\n",
    "    model_label = get_model_label(model_name)\n",
    "    if model_name == 'gpt-3.5-turbo-instruct':\n",
    "        return {model_label: {'model': 'gpt-3.5-turbo-instruct', **llm_hypers}}\n",
    "    elif model_name == 'gpt-4':\n",
    "        return {model_label: {'model': 'gpt-4', **llm_hypers}}\n",
    "    elif model_name == 'gpt-4o':\n",
    "        return {model_label: {'model': 'gpt-4o', **llm_hypers}}\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unexpected model '{model_name}'.\")\n",
    "\n",
    "def get_model_predict_fns(model_name):\n",
    "    model_label = get_model_label(model_name)\n",
    "    if model_name == 'gpt-3.5-turbo-instruct':\n",
    "        return {model_label: get_llmtime_predictions_data}\n",
    "    elif model_name == 'gpt-4':\n",
    "        return {model_label: get_llmtime_predictions_data}\n",
    "    elif model_name == 'gpt-4o':\n",
    "        return {model_label: get_llmtime_predictions_data}\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unexpected model '{model_name}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb55510a-79fe-4993-ac3e-c3f0374bf0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis(asset, w):\n",
    "    window = w + steps\n",
    "\n",
    "    model_hypers = get_model_hypers(model_name)\n",
    "\n",
    "    model_predict_fns = get_model_predict_fns(model_name)\n",
    "\n",
    "    model_names = list(model_predict_fns.keys())\n",
    "\n",
    "    # Delete existing pkl files at the beginning\n",
    "    results_filename = os.path.join(output_directory, 'rolling_results.pkl')\n",
    "    time_log_filename = os.path.join(output_directory, 'time_log.pkl')\n",
    "\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    if os.path.exists(results_filename):\n",
    "        os.remove(results_filename)\n",
    "    if os.path.exists(time_log_filename):\n",
    "        os.remove(time_log_filename)\n",
    "\n",
    "    # Load the DataFrame\n",
    "    df = pd.read_excel(f\"datasets/{asset}.xlsx\")\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "    if df.index.duplicated().any():\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "    # Change start date dynamically\n",
    "    start_index = max(df[df[\"Date\"] == starting_date].index[0] - w - 1, 0)\n",
    "    df = df.iloc[start_index:]\n",
    "    print(df.shape)\n",
    "    # Handle stoxx header\n",
    "    df = df.rename(columns={\"STOXX\": \"Close\"})\n",
    "    df = df.set_index('Date')\n",
    "    df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "    df = df[['Log_Return']]\n",
    "    df = df.dropna()\n",
    "\n",
    "    def create_logger(filename: str, name: str = None):\n",
    "        logger = logging.getLogger(name)\n",
    "    \n",
    "       # Set the overall logging level\n",
    "        logger.setLevel(logging.INFO)\n",
    "    \n",
    "        if len(logger.handlers) == 0:\n",
    "            \n",
    "            # Create handlers\n",
    "            file_handler = logging.FileHandler(filename)\n",
    "            stream_handler = logging.StreamHandler()\n",
    "            \n",
    "            # Set logging levels for handlers\n",
    "            file_handler.setLevel(logging.INFO)\n",
    "            stream_handler.setLevel(logging.INFO)\n",
    "            \n",
    "            # Create formatters and add them to handlers\n",
    "            formatter = logging.Formatter('%(asctime)s | %(filename)s | %(levelname)s | %(message)s')\n",
    "            file_handler.setFormatter(formatter)\n",
    "            stream_handler.setFormatter(formatter)\n",
    "            \n",
    "            # Add handlers to the logger\n",
    "            logger.addHandler(file_handler)\n",
    "            logger.addHandler(stream_handler)\n",
    "    \n",
    "        return logger\n",
    "\n",
    "    def save_results(results, filename=results_filename):\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(results, f)\n",
    "\n",
    "    def load_results(filename=results_filename):\n",
    "        try:\n",
    "            with open(filename, \"rb\") as f:\n",
    "                return pickle.load(f)\n",
    "        except FileNotFoundError:\n",
    "            return []\n",
    "\n",
    "    def save_time_log(start_time, filename=time_log_filename):\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(start_time, f)\n",
    "\n",
    "    def load_time_log(filename=time_log_filename):\n",
    "        try:\n",
    "            with open(filename, \"rb\") as f:\n",
    "                return pickle.load(f)\n",
    "        except FileNotFoundError:\n",
    "            return None\n",
    "\n",
    "    def get_dataset_DP(df):\n",
    "        series = df.iloc[:, 0]\n",
    "        return series\n",
    "\n",
    "    def get_datasets_DP(ds, ds_name, testfrac=0.2, predict_steps=steps):\n",
    "        series = get_dataset_DP(ds)\n",
    "        splitpoint = len(series) - predict_steps if predict_steps is not None else int(len(series) * (1 - testfrac))\n",
    "        train = series.iloc[:splitpoint]\n",
    "        test = series.iloc[splitpoint:]\n",
    "        return {ds_name: (train, test)}\n",
    "\n",
    "    def rolling_window_emergency(df, logger, date, model):\n",
    "        \"\"\"Only used in case of no enough samples generated by LLMTime, which should not happen because of a fix. Predicts only for one model\"\"\"\n",
    "        start = df.reset_index()[df.reset_index()[\"Date\"] == date].index[0]\n",
    "        end = start + w\n",
    "        ds = df.iloc[start:end]\n",
    "        datasets = get_datasets_DP(ds, 'ds', predict_steps=steps)\n",
    "        ds_name = 'ds'\n",
    "        train, test = datasets[ds_name]\n",
    "        random_seed = np.random.randint(0, 100000)\n",
    "        model_hypers[model].update({'dataset_name': ds_name, 'random_seed': random_seed})\n",
    "        hypers = list(grid_iter(model_hypers[model]))\n",
    "        retries = 0\n",
    "        obtained_samples = []\n",
    "        while retries < max_retries and len(obtained_samples) < n:\n",
    "            try:\n",
    "                pred_dict = get_autotuned_predictions_data(train, test, hypers, n, model_predict_fns[model], verbose=False, parallel=True)\n",
    "                obtained_samples = pred_dict['samples']\n",
    "                break\n",
    "            except OpenAIError as e:\n",
    "                retries += 1\n",
    "                sleep(1)\n",
    "                if retries == max_retries:\n",
    "                    logger.error(f\"Failed to get predictions for {model} after {max_retries} retries.\")\n",
    "                    obtained_samples = pd.DataFrame({date: np.random.normal(size=n)})\n",
    "\n",
    "        return obtained_samples\n",
    "\n",
    "    # Main processing function\n",
    "    def rolling_window_datasets(df, logger, window_length=window, predict_steps=steps):\n",
    "        rolling_results = load_results()\n",
    "        start_index = len(rolling_results)\n",
    "        total_iterations = len(df) - window_length\n",
    "\n",
    "        for start in range(start_index, total_iterations):\n",
    "            try:\n",
    "                logger.info(f\"Processing window starting at index {start} of {total_iterations}\")\n",
    "                end = start + window_length\n",
    "                ds = df.iloc[start:end]\n",
    "                datasets = get_datasets_DP(ds, 'ds', predict_steps=steps)\n",
    "                ds_name = 'ds'\n",
    "                train, test = datasets[ds_name]\n",
    "\n",
    "                out = {}\n",
    "                dates = ds.index[-1]\n",
    "                last_value = ds.iloc[-1]\n",
    "\n",
    "                for model in model_names:\n",
    "                    random_seed = np.random.randint(0, 100000)\n",
    "                    model_hypers[model].update({'dataset_name': ds_name, 'random_seed': random_seed})\n",
    "                    hypers = list(grid_iter(model_hypers[model]))\n",
    "\n",
    "                    retries = 0\n",
    "                    while retries < max_retries:\n",
    "                        try:\n",
    "                            pred_dict = get_autotuned_predictions_data(train, test, hypers, n, model_predict_fns[model], verbose=False, parallel=True)\n",
    "                            out[model] = pred_dict['samples']\n",
    "                            break\n",
    "                        except OpenAIError as e:\n",
    "                            retries += 1\n",
    "                            sleep(1)\n",
    "                            if retries == max_retries:\n",
    "                                logger.error(f\"Failed to get predictions for {model} after {max_retries} retries.\")\n",
    "                                out[model] = np.nan\n",
    "\n",
    "                rolling_results.append((dates, out, last_value))\n",
    "\n",
    "                if (start - start_index + 1) % save_interval == 0:\n",
    "                    logger.info(f\"Saving intermediate results at index {start}\")\n",
    "                    save_results(rolling_results)\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error at index {start}: {e}\")\n",
    "                traceback.print_exc()\n",
    "                save_results(rolling_results)\n",
    "                raise e\n",
    "\n",
    "        save_results(rolling_results)\n",
    "        return rolling_results\n",
    "\n",
    "    # Main loop to handle restarting from the last save\n",
    "    logger = create_logger(name=\"llmtime_estimation\", filename=\"llmtime.log\")\n",
    "    while True:\n",
    "        try:\n",
    "            start_time = time()\n",
    "            save_time_log(start_time)\n",
    "            rolling_results = rolling_window_datasets(df, logger)\n",
    "            end_time = time()\n",
    "            logger.info(f\"Processing time: {end_time - start_time} seconds\")\n",
    "\n",
    "            # Post-processing and output\n",
    "            prediction_dict = rolling_results\n",
    "            m = n * steps\n",
    "            column_names = ['Date'] + [f'Log Return {i+1}' for i in range(m)]\n",
    "            output = pd.DataFrame(columns=column_names)\n",
    "        \n",
    "            # Loop through each item in the predictions dictionary\n",
    "            for item in prediction_dict:\n",
    "                date = item[0]  # Extract the date from DatetimeIndex\n",
    "                data_matrix = item[1].get(get_model_label(model_name), None)  # Extract the DataFrame\n",
    "        \n",
    "                if data_matrix is not None and isinstance(data_matrix, pd.DataFrame):\n",
    "                    # Drop missing values\n",
    "                    data_matrix.dropna(inplace=True)\n",
    "                    # Add from existing values if lower than needed\n",
    "                    if data_matrix.shape[0] < n:\n",
    "                        if n - data_matrix.shape[0] < data_matrix.shape[0]:\n",
    "                            data_matrix = pd.concat([data_matrix, data_matrix.sample(n=n - data_matrix.shape[0], axis=0)], axis=0).reset_index()\n",
    "                        else:\n",
    "                            logger.warning(f\"Did not find enough samples for {date.strftime('%Y-%m-%d')}, will try to predict again.\")\n",
    "                            data_matrix = rolling_window_emergency(df=df, date=date, logger=logger, model=model_names[0])\n",
    "                    # Flatten the DataFrame to a single list - keep only n values\n",
    "                    flat_list = data_matrix.values.flatten().tolist()[:n]\n",
    "                    \n",
    "                    # Prepend the date to the list\n",
    "                    row_data = [date] + flat_list\n",
    "                    \n",
    "                    # Append this list as a row in the output DataFrame\n",
    "                    output.loc[len(output)] = row_data\n",
    "                else:\n",
    "                    logger.warning(f\"Warning: Expected DataFrame at {date} but got {type(data_matrix)}\")\n",
    "        \n",
    "            # Write DataFrame to Excel\n",
    "            current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "            # output_filename = f\"{current_date}_{asset}_LLMTime_GPT-3.5_w={w}.xlsx\"\n",
    "            output_filename = os.path.join(output_directory, f\"{current_date}_{asset}_{get_model_label(model_name).replace(' ', '_')}_w={w}.csv\")\n",
    "            output.to_csv(output_filename, index=False)\n",
    "            logger.info(f\"Saved result to {output_filename}.\")\n",
    "\n",
    "            os.remove(results_filename)\n",
    "            os.remove(time_log_filename)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred: {e}. Restarting from last save...\")\n",
    "            sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b17de64-c5c9-44b3-b786-115c51f21be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this cell may take a long time\n",
    "# All logs are stored in 'llmtime.log'\n",
    "for asset in assets:\n",
    "    for w in windows:\n",
    "        run_analysis(asset, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab68011-79b2-456c-874f-5862722afa43",
   "metadata": {},
   "source": [
    "# Test supposed GPT-4 and GPT-4o tokenizers (seem to encode digits separately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f5771-d2aa-4675-9daf-18845e855807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiktoken._educational import *\n",
    "import tiktoken\n",
    "\n",
    "# Train a BPE tokeniser on a small amount of text\n",
    "# enc = train_simple_encoding()\n",
    "\n",
    "# Visualise how the GPT-4 encoder encodes text\n",
    "# enc = SimpleBytePairEncoding.from_tiktoken(\"gpt-4o-2024-05-13\")\n",
    "# enc = SimpleBytePairEncoding.from_tiktoken(\"gpt-3.5-turbo-0301\")\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "# enc.encode(\"hello world aaaaaaaaaaaa\")\n",
    "# enc.decode([15339, 1917])\n",
    "# enc.decode([24912, 2375, 261, 117525])\n",
    "# enc.encode(\"0.1 2 3\")\n",
    "# enc.decode([17, 18])\n",
    "enc.decode(list(map(int, \"356, 405, -467, 494, 258, -534, -252, 905, -1493, -70, -645, 123, -189, 230, 1995, -397, -535, -147, 35, -469, 428, -1119, -1382, -8199, -1382, 492, 459, -584, -832, 2010, -1146, -279, -301, 480, 42, 5245, -1372, -4528, -725, 242, 247, -457, 8, -55, 1255, -855, -75, -591, -3278, 2783, 1146, 1137, 644, -291, -389, 985, 1332, -446, -2276, 280, -119, -1067, 234, -28, 379, 2671, 150, 189, 3597, -2114, -236, 517, -1536, 1853, -188, -115, -1975, -595, -1572, -816, 538, 204, 770, 4003, -736, -294\".split(\", \"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "123d2b4d-0870-4d78-851b-9ee34083cf1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'o200k_base'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tiktoken.encoding_for_model(\"gpt-4o\")"
   ]
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python [conda env:llmtime]",
   "language": "python",
   "name": "conda-env-llmtime-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
